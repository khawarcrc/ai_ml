{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12899018,"sourceType":"datasetVersion","datasetId":8161503}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -------------------------\n# STEP 1 — Install libs (run once)\n# -------------------------\n# On Kaggle, transformers/datasets are usually preinstalled.\n# To avoid dependency conflicts, install only missing extras:\n!pip install -q soundfile librosa gradio\n\n# If you truly need to force-update transformers/torch (not recommended on Kaggle),\n# uncomment the next line (will be large and may cause CUDA/version warnings).\n# !pip install -q --upgrade transformers datasets torch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:37:37.345226Z","iopub.execute_input":"2025-08-28T18:37:37.346163Z","iopub.status.idle":"2025-08-28T18:37:41.409259Z","shell.execute_reply.started":"2025-08-28T18:37:37.346122Z","shell.execute_reply":"2025-08-28T18:37:41.408293Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------\n# STEP 2 — Imports & logging (run after STEP 1)\n# -------------------------\n# Import necessary packages and silence verbose logs so output is clean.\nfrom transformers.utils import logging\nlogging.set_verbosity_error()    # reduce noise\n\nimport os\nimport numpy as np\nimport librosa\nimport soundfile as sf           # used for saving or reading certain formats\nfrom transformers import pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:39:10.222525Z","iopub.execute_input":"2025-08-28T18:39:10.222855Z","iopub.status.idle":"2025-08-28T18:39:41.492217Z","shell.execute_reply.started":"2025-08-28T18:39:10.222828Z","shell.execute_reply":"2025-08-28T18:39:41.491375Z"}},"outputs":[{"name":"stderr","text":"2025-08-28 18:39:22.935459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756406363.159001      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756406363.222583      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# STEP 3 — Confirm your uploaded file path\n# -------------------------\n# List files inside /kaggle/input so you know exact filenames/paths.\nroot = \"/kaggle/input\"\nfor path, dirs, files in os.walk(root):\n    if files:\n        print(\"Folder:\", path)\n        for f in files:\n            print(\"  -\", f)\n\n# Now set the exact file path you saw printed above:\nfile_path = \"/kaggle/input/my-voice-input/voice_input.m4a\"   # <-- update if different\nprint(\"\\nUsing file_path =\", file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:40:20.759551Z","iopub.execute_input":"2025-08-28T18:40:20.759858Z","iopub.status.idle":"2025-08-28T18:40:20.768456Z","shell.execute_reply.started":"2025-08-28T18:40:20.759837Z","shell.execute_reply":"2025-08-28T18:40:20.767727Z"}},"outputs":[{"name":"stdout","text":"Folder: /kaggle/input/my-voice-input\n  - voice_input.m4a\n\nUsing file_path = /kaggle/input/my-voice-input/voice_input.m4a\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -------------------------\n# STEP 4 — Load the ASR model (this downloads from Hugging Face)\n# -------------------------\n# Creates a pipeline object `asr` that you will call with audio arrays.\n# Note: the first run downloads model weights and may take a minute.\nasr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n\n# Inspect what sampling rate the model expects (usually 16000 for Whisper)\nprint(\"Model expected sampling rate:\", asr.feature_extractor.sampling_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:40:28.936111Z","iopub.execute_input":"2025-08-28T18:40:28.936803Z","iopub.status.idle":"2025-08-28T18:40:35.390580Z","shell.execute_reply.started":"2025-08-28T18:40:28.936777Z","shell.execute_reply":"2025-08-28T18:40:35.389821Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e7669471bc44a0828fd56de6a0ca96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd16b1704094a00a8b5d7d1bedcacd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"014c3ad9894f41828ecb2008333d82e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc651ddd8e4e454e8d6e0a9699fb2f3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b0e6430b4043fd9d6de591f49fe00b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372e0980dc4b48fea39b2c892c4276bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482a0c2a07e54540b1acbe55db247747"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac54f77a91544ea9b9d9c3648b00f3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31534660e93a4c6fa87090cce4c4d7d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27507cd25b604894862f80185b2bb58c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34172d729be74a4ca35989c9cced6a35"}},"metadata":{}},{"name":"stdout","text":"Model expected sampling rate: 16000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -------------------------\n# STEP 5 — Load audio (handles .m4a, mp3, wav)\n# -------------------------\n# librosa.load uses audioread as a fallback for formats libsndfile can't read.\n# sr=None keeps the original sample rate (we will resample to model_sr later).\naudio, sr = librosa.load(file_path, sr=None, mono=True)  # mono=True -> combine channels\nprint(\"Loaded audio: shape =\", getattr(audio, \"shape\", None), \"sr =\", sr)\n\n# If librosa couldn't read the format, you may need ffmpeg or convert to WAV offline.\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:41:47.800584Z","iopub.execute_input":"2025-08-28T18:41:47.800923Z","iopub.status.idle":"2025-08-28T18:42:02.558323Z","shell.execute_reply.started":"2025-08-28T18:41:47.800898Z","shell.execute_reply":"2025-08-28T18:42:02.557493Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1351363508.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n  audio, sr = librosa.load(file_path, sr=None, mono=True)  # mono=True -> combine channels\n/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","output_type":"stream"},{"name":"stdout","text":"Loaded audio: shape = (797696,) sr = 48000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# -------------------------\n# STEP 6 — Preprocess: ensure mono, resample, dtype\n# -------------------------\n# Make sure audio is 1D numpy array (mono). If not, convert.\nif getattr(audio, \"ndim\", 1) > 1:\n    # librosa.load with mono=True normally avoids this, but check defensively\n    audio = librosa.to_mono(audio.T)\n\nmodel_sr = asr.feature_extractor.sampling_rate\nif sr != model_sr:\n    audio = librosa.resample(audio, orig_sr=sr, target_sr=model_sr)\n    sr = model_sr\n    print(f\"Resampled to {sr} Hz\")\n\n# Convert to float32 which most pipelines expect and keep values in [-1,1]\naudio = audio.astype(np.float32)\n\n# Quick safety checks\nduration_s = len(audio) / sr\nprint(f\"Final audio shape: {audio.shape}, sr: {sr}, duration: {duration_s:.2f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:43:29.836675Z","iopub.execute_input":"2025-08-28T18:43:29.838092Z","iopub.status.idle":"2025-08-28T18:43:29.853752Z","shell.execute_reply.started":"2025-08-28T18:43:29.838055Z","shell.execute_reply":"2025-08-28T18:43:29.852776Z"}},"outputs":[{"name":"stdout","text":"Resampled to 16000 Hz\nFinal audio shape: (265899,), sr: 16000, duration: 16.62s\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# -------------------------\n# STEP 7 — Transcribe (safe handling for short & long audio)\n# -------------------------\n# Choose chunk_length_s and batch_size depending on audio length and memory.\n# chunk_length_s splits long audio so model processes in manageable pieces.\nchunk_length_s = 30\nbatch_size = 4\n\n# Run the ASR. For short audios this returns {'text': ...}.\n# For longer audio or when return_timestamps=True you may also get 'chunks'.\nresult = asr(audio, chunk_length_s=chunk_length_s, batch_size=batch_size, return_timestamps=True)\n\n# Print raw result for inspection\nprint(\"Raw pipeline output:\\n\", result)\n\n# Extract final transcript:\nif isinstance(result, dict) and \"chunks\" in result:\n    # Join chunk texts (keeps timestamps if you need them)\n    transcript = \" \".join(chunk[\"text\"].strip() for chunk in result[\"chunks\"])\nelse:\n    transcript = result.get(\"text\") if isinstance(result, dict) else str(result)\n\nprint(\"\\n=== Final transcript ===\\n\", transcript)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:43:44.450157Z","iopub.execute_input":"2025-08-28T18:43:44.450941Z","iopub.status.idle":"2025-08-28T18:43:50.130224Z","shell.execute_reply.started":"2025-08-28T18:43:44.450911Z","shell.execute_reply":"2025-08-28T18:43:50.129457Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Raw pipeline output:\n {'text': \" well let's see what we are trying to do we are implementing automatic speech legalization and currently I am recording my voice for giving as an input well let's see what we can get with this model\", 'chunks': [{'timestamp': (0.0, 5.26), 'text': \" well let's see what we are trying to do we are implementing automatic speech\"}, {'timestamp': (5.26, 12.6), 'text': ' legalization and currently I am recording my voice for giving as an input'}, {'timestamp': (12.6, 17.38), 'text': \" well let's see what we can get with this model\"}]}\n\n=== Final transcript ===\n well let's see what we are trying to do we are implementing automatic speech legalization and currently I am recording my voice for giving as an input well let's see what we can get with this model\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# -------------------------\n# STEP 8 — (Optional) Save processed mono/resampled WAV for download\n# -------------------------\n# Save into /kaggle/working so you can download from the notebook output.\nout_wav = \"/kaggle/working/voice_input_processed.wav\"\nsf.write(out_wav, audio, sr)   # soundfile writes float32 or PCM automatically\nprint(\"Saved processed WAV to:\", out_wav)\n# You can download the file via the Notebook -> Output -> Files section.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:44:11.712243Z","iopub.execute_input":"2025-08-28T18:44:11.713073Z","iopub.status.idle":"2025-08-28T18:44:11.730376Z","shell.execute_reply.started":"2025-08-28T18:44:11.713044Z","shell.execute_reply":"2025-08-28T18:44:11.729259Z"}},"outputs":[{"name":"stdout","text":"Saved processed WAV to: /kaggle/working/voice_input_processed.wav\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# -------------------------\n# STEP 9 — (Optional) Quick Gradio UI to record or upload audio (run last)\n# -------------------------\nimport gradio as gr\n\ndef transcribe_file(filepath):\n    if filepath is None:\n        return \"No file provided.\"\n    # Pass file path directly to the pipeline\n    out = asr(filepath, chunk_length_s=30, batch_size=4, return_timestamps=False)\n    return out.get(\"text\", str(out))\n\n# Gradio Audio component (no \"source\" argument in latest Gradio)\niface = gr.Interface(\n    fn=transcribe_file,\n    inputs=gr.Audio(type=\"filepath\"),   # works for both microphone & upload\n    outputs=gr.Textbox(lines=5, label=\"Transcription\"),\n    title=\"Record or Upload and Transcribe\"\n)\n\n# Launch the UI\niface.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:45:32.754652Z","iopub.execute_input":"2025-08-28T18:45:32.754974Z","iopub.status.idle":"2025-08-28T18:45:35.773260Z","shell.execute_reply.started":"2025-08-28T18:45:32.754951Z","shell.execute_reply":"2025-08-28T18:45:35.772464Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://269fb5a6828b06630c.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://269fb5a6828b06630c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"del asr\nimport gc, torch\ngc.collect()\ntry:\n    torch.cuda.empty_cache()\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:46:38.643429Z","iopub.execute_input":"2025-08-28T18:46:38.643763Z","iopub.status.idle":"2025-08-28T18:46:38.677098Z","shell.execute_reply.started":"2025-08-28T18:46:38.643731Z","shell.execute_reply":"2025-08-28T18:46:38.675817Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3766802915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0masr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'asr' is not defined"],"ename":"NameError","evalue":"name 'asr' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# ------------------------------------------------------------\n# PROBLEM STATEMENT:\n# ------------------------------------------------------------\n# We wanted to automatically convert speech (audio) into text \n# in Kaggle/Colab.\n# In simple words: \n#   \"Given an audio file (or recording), transcribe the spoken \n#    words into text using Python.\"\n\n# ------------------------------------------------------------\n# SOLUTION STEPS:\n# ------------------------------------------------------------\n\n# 1. MODEL SETUP\n#    - Loaded a pretrained speech-to-text model (Whisper) from Hugging Face.\n#    - This model is already trained to recognize speech and output text.\n\n# 2. AUDIO INPUT HANDLING\n#    - Allowed audio input in two ways:\n#        a) Uploading a file (e.g., .wav, .mp3, .m4a).\n#        b) Recording directly from the microphone (via Gradio UI).\n\n# 3. TRANSCRIPTION PIPELINE\n#    - Built a function that:\n#        -> Takes the audio file as input.\n#        -> Feeds it to the Whisper ASR pipeline.\n#        -> Returns the recognized text.\n\n# 4. OPTIMIZATION FOR LONG AUDIO\n#    - Used chunking (chunk_length_s=30, batch_size=4).\n#    - This breaks long audio into smaller pieces so the model\n#      processes them safely without running out of memory.\n\n# 5. MEMORY MANAGEMENT\n#    - Freed up memory after usage with:\n#        -> gc.collect()\n#        -> torch.cuda.empty_cache()\n#    - Useful if reloading or switching models.\n\n# 6. GRADIO UI (Optional but User-Friendly)\n#    - Built a small interface with Gradio where:\n#        -> Users can record audio or upload a file.\n#        -> The transcribed text is shown in a textbox.\n#    - Makes the solution easier for non-programmers to use.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T18:50:08.313226Z","iopub.execute_input":"2025-08-28T18:50:08.313594Z","iopub.status.idle":"2025-08-28T18:50:08.320045Z","shell.execute_reply.started":"2025-08-28T18:50:08.313570Z","shell.execute_reply":"2025-08-28T18:50:08.319010Z"}},"outputs":[],"execution_count":15}]}